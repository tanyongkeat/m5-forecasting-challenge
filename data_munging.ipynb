{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import datetime\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pmdarima\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from fbprophet import Prophet\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(f'{input_path}calendar.csv', parse_dates=['date'])\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downcast_dict = {'wm_yr_wk': np.int16,\n",
    "                'wday': np.int16,\n",
    "                'd': 'category',\n",
    "                'snap_CA': np.uint8,\n",
    "                'snap_TX': np.uint8,\n",
    "                'snap_WI': np.uint8,\n",
    "                'event': np.uint8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy variables are created for each event type or event name by testing their presence in $event\\_name/type\\_1$ and $event\\_name/type\\_2$. After some exploration, I find out that if there is any event 2, there will definitely be event 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_types = calendar.event_type_1.unique()[1:]\n",
    "\n",
    "for event_type in event_types:\n",
    "    calendar['event_' + event_type.lower()] = ((calendar.event_type_1 == event_type) | (calendar.event_type_2 == event_type)).map({True: 1, False: 0})\n",
    "    downcast_dict['event_' + event_type.lower()] = np.uint8\n",
    "event_names = calendar.event_name_1.unique()[1:]\n",
    "for event_name in event_names:\n",
    "    calendar['event_' + event_name.lower()] = ((calendar.event_name_1 == event_name) | (calendar.event_name_2 == event_name)).map({True: 1, False: 0})\n",
    "    downcast_dict['event_' + event_name.lower()] = np.uint8\n",
    "    \n",
    "calendar['event'] = (~calendar.event_name_1.isnull()).map({True: 1, False: 0}) #dummy variable to test the presence of any event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnecessary fields are dropped and other fields are downcasted to save RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.drop(['weekday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'], axis=1, inplace=True)\n",
    "calendar = calendar.astype(downcast_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output as an object to retain all the types downcasted. (feature can be a better option because its performance is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_path}processed_calendar', 'wb+') as out:\n",
    "    pickle.dump(calendar, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(f'{input_path}sell_prices.csv')\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the fields into one $id$ to match the $id$ in submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices['id'] = prices.item_id + '_' + prices.store_id + '_validation'\n",
    "prices.drop(['store_id', 'item_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = prices.astype({'id': 'category', \n",
    "               'wm_yr_wk': np.int16,\n",
    "               'sell_price': np.float16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_path}processed_prices', 'wb+') as out:\n",
    "    pickle.dump(prices, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(f'{input_path}sales_train_validation.csv')\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['id'] = 'category'\n",
    "\n",
    "for i in range(1, 1914):\n",
    "    d['d_' + str(i)] = np.int16\n",
    "\n",
    "sales = sales.astype(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melt the sales dataframe so that the days fields can be used to merge with other dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "melted_sales = pd.melt(sales, id_vars='id', var_name='d', value_name='demand')\n",
    "print(\"sales melted {}\".format(time.time()-st))\n",
    "\n",
    "melted_sales['d'] = melted_sales['d'].astype('category')\n",
    "\n",
    "st = time.time()\n",
    "with open(f'{output_path}melted_sales', 'wb+') as out:\n",
    "    pickle.dump(melted_sales, out)\n",
    "time.time()-st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merging all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "prices_calendar = pd.merge(prices, calendar, on='wm_yr_wk', how='left')\n",
    "print('prices merged with calendar {}'.format(time.time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "full_df = pd.merge(prices_calendar, melted_sales, on=['id', 'd'], how='left')\n",
    "print('merged with melted sales {}'.format(time.time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.set_index('date')\n",
    "full_df = full_df.astype({'d':'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{output_path}full_df', 'wb+') as out:\n",
    "    pickle.dump(full_df, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
